{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyPDF2\n",
      "  Obtaining dependency information for pyPDF2 from https://files.pythonhosted.org/packages/8e/5e/c86a5643653825d3c913719e788e41386bee415c2b87b4f955432f2de6b2/pypdf2-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 30.7/232.6 kB 262.6 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 41.0/232.6 kB 279.3 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 174.1/232.6 kB 952.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 232.6/232.6 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pyPDF2\n",
      "Successfully installed pyPDF2-3.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of pages:  35\n",
      " \n",
      " \n",
      " Development  Plan for Greater Mumbai 2014‐2034                                                                                                                                                                                                                                                      \n",
      "Acknowledgements  \n",
      "The Consultant  wishes to thank the following  individuals  from the Municipal  Corporation  of \n",
      "Greater Mumbai for their invaluable  support, insights and contributions  towards ‘Working  Paper 1 \n",
      "– Preparation  of Base Map’ for the preparation  of the Development  Plan for Greater Mumbai \n",
      "2014‐34. \n",
      " Mr. Subodh Kumar, IAS, Municipal  Commissioner;  \n",
      " Mr. Rajeev Kuknoor, Chief Engineer Development  Plan; \n",
      " Mr. Sudhir Ghate, Deputy Chief Engineer Development  Plan; \n",
      " Mr. A.G. Marathe, Deputy Chief Engineer Development  Plan; \n",
      " Mr. R. Balachandran,  Executive  Engineer and Town Planning Officer, Development  Plan. \n",
      " Our gratitude  to the following  experts for their invaluable  insights and support: \n",
      " \n",
      "Mr. V.K Phatak, Former Chief Town Planner (MMRDA);  \n",
      " Mr. A.N Kale, Former Chief Engineer, (DP); \n",
      " Mr. A. S Jain Former Dy. Chief Engineer, (DP). \n",
      " We wish to especially  thank MCGM officers, Mr. Jagdish Talreja, Mr. Dinesh Naik, Mr. Hiren \n",
      "Daftardar,  Ms. Anita Naik for their continual  support since the\n",
      " beginning  of the project and their \n",
      "help towards familiarization  and data collection.  They have been instrumental  in helping to \n",
      "contact various MCGM departments  as well as in helping to establish contact with personnel  from \n",
      "other government  departments  and organizations.  Many thanks for the MCGM team, for \n",
      "deploying  personnel,  particularly  Mr. Prasad Gharat, on extensive  field visits that have helped in \n",
      "understanding  actual ground conditions.  \n",
      " \n",
      "We apologize  if we have inadvertently  omitted anyone to whom acknowledgement  is due. We hope \n",
      "and anticipate  the work's usefulness  for the intended purpose. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "pdf = open(\"file.pdf\", 'rb')\n",
    "\n",
    "pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "\n",
    "print(\"No of pages: \", len(pdf_reader.pages))\n",
    "\n",
    "page = pdf_reader.pages[1]\n",
    "\n",
    "print(page.extract_text())\n",
    "\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2, urllib, nltk\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "\n",
    "wFile = urllib.request.urlopen('http://www.udri.org/pdf/02%20working%20paper%201.pdf')\n",
    "pdfreader = PyPDF2.PdfReader(BytesIO(wFile.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageObj = pdfreader.pages[2]\n",
    "page2 = pageObj.extract_text()\n",
    "punctuations=['(',')',';',':','[',']',',','...','.']\n",
    "tokens=word_tokenize(page2)\n",
    "stop_words=stopwords.words('english')\n",
    "keywords=[word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2014‐2034',\n",
       " 'Table',\n",
       " 'Contents',\n",
       " 'The',\n",
       " 'Consultant',\n",
       " 'wishes',\n",
       " 'thank',\n",
       " 'following',\n",
       " 'individuals',\n",
       " 'Municipal',\n",
       " 'Corporation',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " 'invaluable',\n",
       " 'support',\n",
       " 'insights',\n",
       " 'contributions',\n",
       " 'towards',\n",
       " '‘',\n",
       " 'Working',\n",
       " 'Paper',\n",
       " '1',\n",
       " '–',\n",
       " 'Preparation',\n",
       " 'Base',\n",
       " 'Map',\n",
       " '’',\n",
       " 'preparation',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2014‐34',\n",
       " '.............................................................................................................................',\n",
       " '..............',\n",
       " '3',\n",
       " 'Our',\n",
       " 'gratitude',\n",
       " 'following',\n",
       " 'experts',\n",
       " 'invaluable',\n",
       " 'insights',\n",
       " 'support',\n",
       " '............................',\n",
       " '3',\n",
       " 'We',\n",
       " 'wish',\n",
       " 'especially',\n",
       " 'thank',\n",
       " 'MCGM',\n",
       " 'officers',\n",
       " 'Mr.',\n",
       " 'Jagdish',\n",
       " 'Talreja',\n",
       " 'Mr.',\n",
       " 'Dinesh',\n",
       " 'Naik',\n",
       " 'Mr.',\n",
       " 'Hiren',\n",
       " 'Daftardar',\n",
       " 'Ms.',\n",
       " 'Anita',\n",
       " 'Naik',\n",
       " 'continual',\n",
       " 'support',\n",
       " 'since',\n",
       " 'beginning',\n",
       " 'project',\n",
       " 'help',\n",
       " 'towards',\n",
       " 'familiarization',\n",
       " 'data',\n",
       " 'collection',\n",
       " 'They',\n",
       " 'instrumental',\n",
       " 'helping',\n",
       " 'contact',\n",
       " 'various',\n",
       " 'MCGM',\n",
       " 'departments',\n",
       " 'well',\n",
       " 'helping',\n",
       " 'establish',\n",
       " 'contact',\n",
       " 'personnel',\n",
       " 'government',\n",
       " 'departments',\n",
       " 'organizations',\n",
       " 'Many',\n",
       " 'thanks',\n",
       " 'MCGM',\n",
       " 'team',\n",
       " 'deploying',\n",
       " 'personnel',\n",
       " 'particularly',\n",
       " 'Mr.',\n",
       " 'Prasad',\n",
       " 'Gharat',\n",
       " 'extensive',\n",
       " 'field',\n",
       " 'visits',\n",
       " 'helped',\n",
       " 'understanding',\n",
       " 'actual',\n",
       " 'ground',\n",
       " 'conditions',\n",
       " '........................................................................................',\n",
       " '3',\n",
       " 'BEST',\n",
       " '...............................................................................................................................',\n",
       " '.................',\n",
       " '5',\n",
       " 'Brihanmumbai',\n",
       " 'Electric',\n",
       " 'Supply',\n",
       " 'Transport',\n",
       " 'Undertaking',\n",
       " '..............................................................',\n",
       " '5',\n",
       " 'CIDCO',\n",
       " '...............................................................................................................................',\n",
       " '..............',\n",
       " '5',\n",
       " 'City',\n",
       " 'Industrial',\n",
       " 'Development',\n",
       " 'Corporation',\n",
       " '...............................................................................',\n",
       " '5',\n",
       " 'CTP',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Comprehensive',\n",
       " 'Transportation',\n",
       " 'Plan',\n",
       " '...............................................................................................',\n",
       " '5',\n",
       " 'DP',\n",
       " '...............................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " '..........................................................................................................................',\n",
       " '5',\n",
       " 'DPGM34',\n",
       " '...............................................................................................................................',\n",
       " '..........',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2034',\n",
       " '.......................................................................................',\n",
       " '5',\n",
       " 'DCR',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Control',\n",
       " 'Regulations',\n",
       " '...................................................................................................',\n",
       " '5',\n",
       " 'DGPS',\n",
       " '...........................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Digital',\n",
       " 'Global',\n",
       " 'Positioning',\n",
       " 'System',\n",
       " '...................................................................................................',\n",
       " '5',\n",
       " 'DPGM',\n",
       " '...............................................................................................................................',\n",
       " '..............',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '...........................................................................................',\n",
       " '5',\n",
       " 'ELU',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Existing',\n",
       " 'Land',\n",
       " 'use',\n",
       " '.............................................................................................................................',\n",
       " '5',\n",
       " 'FSI',\n",
       " '...............................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Floor',\n",
       " 'Space',\n",
       " 'Index',\n",
       " '............................................................................................................................',\n",
       " '5',\n",
       " 'GIS',\n",
       " '...............................................................................................................................',\n",
       " '...................',\n",
       " '5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.Jagdish Talreja', 'Mr.Dinesh Naik', 'Mr.Hiren Daftardar', 'Ms.Anita Naik', 'Mr.Prasad Gharat']\n"
     ]
    }
   ],
   "source": [
    "name_list=list()\n",
    "check = ['Mr','Mrs.','Ms.']\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token.startswith(tuple(check)) and idx<(len(tokens)-1):\n",
    "        name=token+tokens[idx+1]+ ' ' + tokens[idx+2]\n",
    "        name_list.append(name)\n",
    "print(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Obtaining dependency information for python-docx from https://files.pythonhosted.org/packages/3e/3d/330d9efbdb816d3f60bf2ad92f05e1708e4a1b9abe80461ac3444c83f749/python_docx-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting lxml>=3.1.0 (from python-docx)\n",
      "  Obtaining dependency information for lxml>=3.1.0 from https://files.pythonhosted.org/packages/fc/82/ace5a5676051e60355bd8fb945df7b1ba4f4fb8447f2010fb816bfd57724/lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\kalyani\\onedrive\\desktop\\nlp\\myenv\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/244.3 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/244.3 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/244.3 kB 259.2 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 30.7/244.3 kB 259.2 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 41.0/244.3 kB 195.7 kB/s eta 0:00:02\n",
      "   ------ -------------------------------- 41.0/244.3 kB 195.7 kB/s eta 0:00:02\n",
      "   ------ -------------------------------- 41.0/244.3 kB 195.7 kB/s eta 0:00:02\n",
      "   ------ -------------------------------- 41.0/244.3 kB 195.7 kB/s eta 0:00:02\n",
      "   ------ -------------------------------- 41.0/244.3 kB 195.7 kB/s eta 0:00:02\n",
      "   ----------- --------------------------- 71.7/244.3 kB 151.0 kB/s eta 0:00:02\n",
      "   ----------- --------------------------- 71.7/244.3 kB 151.0 kB/s eta 0:00:02\n",
      "   ---------------------- --------------- 143.4/244.3 kB 250.2 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 235.5/244.3 kB 411.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 244.3/244.3 kB 404.3 kB/s eta 0:00:00\n",
      "Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.4/3.8 MB 9.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.9/3.8 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.2/3.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.9/3.8 MB 8.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.8/3.8 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml, python-docx\n",
      "Successfully installed lxml-5.3.0 python-docx-1.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=open('DL.docx','rb')\n",
    "document=docx.Document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can Convolutional Neural Networks (CNNs) be used to extract high-level features from image and video data, and what are some of the key technical challenges involved in this process?Convolutional Neural Networks (CNNs) are a type of deep learning model that are highly effective for image and video processing. They automatically learn hierarchical representations of visual data, allowing them to extract both low-level and high-level features. MLP‟s use one perceptron for each input (e.g. pixel in an image, multiplied by 3 in RGB case). The amount of weights rapidly becomes unmanageable for large images. For a 224 x 224 pixel image with 3 colour channels there are around 150,528 weights that must be trained. As a result, difficulties arise while training and overfitting can occur. There are several techniques used to avoid overfitting in CNNs, some of which are specific to CNN architectures such as Regularization (L1/L2),  Dropout, Batch normalization, Early stopping etc. The Convolutional neural network (CNN) model for image and video processing involves the following steps.A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and is used mainly for image processing, classification, and segmentation. Input layer: The input to a cnn, is mostly an image( nxmx1-gray scale image and nxmx3- colored image).Convolution layer: it basically defines filters and we compute the convolution between the defined filters and each of the 3 images.In the same way it is applied to the remaining image i.e blue and green for RGB image (above is for red image). It is possible to apply more than one filter. More filters can be used to preserve better spatial dimensions. Convolution is used instead of considering flatten images as input to optimise the parameter and to reduce computational expense. Eg. We require 25 weights if we take a 5x5x1 image without convolution. We require 16 weights (n-f+1 x n-f+1) if we take a 5x5x1 image with a 2x2 convolution filter. Also by using convolution we can prevent overfitting of the model. It is worth having the ReLU activation function in the convolution layer which passes only positive values and negative values to zeros. Pooling layer: Pooling layer objective is to reduce the spatial dimensions of the data propagating through the network. There can be different types of pooling applied to CNN model1. Max Pooling is the most common, for each section of the image we scan and keep the highest value. Max. Pooling provides spatial variance which enables the neural network to recognize objects in an image even if the object does not exactly resemble the original object. 2 2. Average Pooling: Here, we take average of area we scan.Fully Connected Layer: Here, we flatten the output of the last convolutional layer and connect every node of the current layer with every other node of the next layer. This layer basically takes output of the preceding layer, whether it is a convolutional layer, ReLU or Pooling layer and outputs an n-dimensional vector, where n is the number of classes pertaining to the problem. Fig. Fully Connected LayerDiscuss the significance of using advanced activation functions in neural networks. How do alternatives like ReLU and variants address the limitations of traditional functions? Activation Functions are an extremely important feature of the Artificial Neural Network. They basically decide whether a neuron should be activated or not. It limits the output signal to a finite value.  Activation Function does the non-linear transformation to the input making it capable to learn more complex relation between input and output. It makes the network capable of learning more complex patterns. Without an activation function, the neural network is just a linear regression model as it performs only summation of product of input and weights. Activation function must be efficient and it should reduce the computation time because the neural network is sometimes trained on millions of data points.The Activation Functions can be basically divided into 3 types1. Binary step Activation Function 2. Linear Activation Function 3. Non-linear Activation FunctionsReLU (Rectified Linear Unit) activation is important in neural networks for several reasons, especially when compared to other activation functions like sigmoid, tanh, or even variants like Leaky ReLU.ReLU is widely used because it is simple, computationally efficient, helps mitigate the vanishing gradient problem, and generally leads to faster training and better performance in deep neural networks.ReLU(Rectified Linear Unit): The ReLU is the most used activation function. It is used in almost all convolutional neural networks in hidden layers only. The ReLU is half rectified(from bottom). f(z) = 0, if z < 0          = z, otherwise R(z) = max(0,z) The range is 0 to inf. Advantages  Avoids vanishing gradient problem.  Computationally efficient—allows the network to converge very quicklyNon-linear—although it looks like a linear function, ReLU has a derivative function and allows for backpropagation.Disadvantages  It can only be used with a hidden layer  Hard to train on small datasets and need much data for learning nonlinear behaviour..  The Dying ReLU problem—when inputs approach zero, or are negative, the gradient of the function becomes zero, the network cannot perform backpropagation and cannot learn.All the negative values are converted into zero, and this conversion rate is so fast that neither it can map nor fit into data properly which creates a problem.Leaky ReLU Activation Function The Leaky ReLU activation function solves the “Dying ReLU‟ problem. Leaky ReLU do not make all negative inputs to zero but to a value near to zero which solves the major issue of ReLU activation function. Advantages  Prevents dying ReLU problem—this variation of ReLU has a small positive slope in the negative area, so it does enable backpropagation, even for negative input values  Otherwise like ReLU Disadvantages  Results not consistent—leaky ReLU does not provide consistent predictions for negative input values.Explain Long Short term Memory (LSTM). What are some of the key technical challenges involved in this process?Steps Involved in LSTM Networks: Step 1: Decide how much past data it should remember The first step in the LSTM is to decide which information should be omitted from the cell in that particular time step. The sigmoid function determines this. It looks at the previous state (ht-1) along with the current input xt and computes the function.Step 2: Decide how much this unit adds to the current stateIn the second layer, there are two parts. One is the sigmoid function, and the other is the tanh function. In the sigmoid function, it decides which values to let through (0 or 1). tanh function gives weightage to the values which are passed, deciding their level of importance (-1 to 1). Step 3: Decide what part of the current cell state makes it to the outputThe third step is to decide what the output will be. First, we run a sigmoid layer, which decides what parts of the cell state make it to the output. Then, we put the cell state through tanh to push the values to be between -1 and 1 and multiply it by the output of the sigmoid gate. How does Batch Normalization work, and how can we use this technique to improve the training stability and generalization performance of Deep Learning models?Batch Normalization is a normalization technique done between the layers of a Neural Network instead of in the raw data. It is done along mini-batches instead of the full data set. It serves to speed up training and use higher learning rates, making learning easier.   Normalization formula of Batch Normalization is defined  as:Where mz is the mean of the neurons’ output and szt is the standard deviation of the neurons’ output. Thus Normalization is the process of altering the input data to have mean as zero and standard deviation value as one.Procedure to do Batch Normalization: Step 1:-Consider the batch input from layer h, for this layer we need to calculate the mean of this hidden activation. Step 2:-After calculating the mean the next step is to calculate the standard deviation of the hidden activations. Step 3:-Now we normalize the hidden activations using these Mean & Standard Deviation values. To do this, we subtract the mean from each input and divide the whole value with the sum of standard deviation and the smoothing term (ε). Step 4:-As the final stage, the re-scaling and offsetting of the input is performed. Here two components of the BN algorithm are used, γ(gamma) and β (beta). These parameters are used for re-scaling (γ) and shifting(β) the vector containing values from the previous operations. These two parameters are learnable parameters, Hence during the training of neural networks, the optimal values of γ and β are obtained and used. Hence we get the accurate normalization of each batch.Batch Normalization improve the training stability and generalization performance of Deep Learning models with the help of its several benefits, which includesIt enables higher learning rates: In a non-normalized network, a large learning rate can lead to oscillations and cause the loss function increase rather than decrease. Batch Normalization helps prevents these problems by preventing small changes in the parameters from amplifying into larger and sub-optimal changes in activations and gradients. Higher learning rates in turn speed up the training process considerably. It enables better Gradient Propagation through the network, thus enabling DLNs with more hidden layers. It helps to reduce strong dependencies on the parameter initialization values. It helps to regularize the model.  Batch Normalization reduces the need to use other regularization techniques such     as Dropout.What are some of the key advantages and limitations of popular activation functions and how can we develop better activation functions that are more effective for specific applications? Neurons are responsible for making decisions. According to the activation function, the neurons determine whether to make a linear or nonlinear decision. Neurons can not learn with just a linear function attached to it. A non-linear activation function learns as per the difference w.r.t error. The main terminologies needed to understand for nonlinear functions are: Derivative or Differential: Change in y-axis w.r.t. change in x-axis.It is also known as slope. Monotonic function: A function which is either entirely non-increasing or non-decreasing.A nonlinear function which is both differential and monotonic is a better activation function and it can be  more effective for specific applications.Non-linear activation functions are crucial in neural networks because they enable the network to model complex, non-linear relationships between the input and output, allowing the network to learn more sophisticated patterns. The most commonly used non-linear activation functions are1. Sigmoid(Logistic AF)(σ): The main reason to use the sigmoid function is it exists between 0 to 1. It is especially used for models where we have to predict the probability as output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.AdvantagesEasy to understand and apply Easy to train on small dataset Smooth gradient, preventing “jumps” in output values. Output values bound between 0 and 1, normalizing the output of each neuron.Disadvantages:  Vanishing gradient—for very high or very low values of X, there is almost no change to the prediction, causing a vanishing gradient problem. This can result in the network refusing to learn further, or being too slow to reach an accurate prediction.  Outputs not zero centered.  Computationally expensive 2. TanH(Hyperbolic Tangent AF)TanH is also like logistic sigmoid but in a better way. The range of the TanH function is from -1 to +1. TanH is often preferred over the sigmoid neuron because it is zero centred. The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in tanh graph.Advantages  Zero centered—making it easier to model inputs that have strongly negative, neutral, and strongly positive values. Disadvantages  Like the Sigmoid function is also suffers from vanishing gradient problem  hard to train on small datasets3. Softmax:  Softmax is an activation function that scales numbers/logits into probabilities. The output of a Softmax is a vector with probabilities of each possible outcome. The probabilities in vector sums to one for all possible outcomes or classes. Sigmoid is able to handle more than two cases(class label).  Softmax can handle multiple cases. Softmax function squeezes the output for each class between 0 and 1 with sum of them is 1.  Advantages Able to handle multiple classes only one class in other activation functions—normalizes the outputs for each class between 0 and 1 with the sum of the probabilities being equal to 1, and divides by their sum, giving the probability of the input value being in a specific class. Useful for output neurons—typically Softmax is used only for the output layer, for neural networks that need to classify inputs into multiple categories.Discuss the concept of early stopping as a regularization technique in deep learning?Early stopping is one of the most commonly used strategies because it is very simple and quite effective. It refers to the process of stopping the training when the training error is no longer decreasing but the validation error is starting to rise.Early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. This implies the process of storing the trainable parameters periodically and tracking the validation error. After the training stops, it returns the trainable parameters to the exact point where the validation error started to rise, instead of the last ones. A different way to think of early stopping is as a very efficient hyperparameter selection algorithm, which sets the number of epochs to the absolute best. It essentially restricts the optimization procedure to a small volume of the trainable parameters space close to the initial parametersIt can also be proven that in the case of a simple linear model with a quadratic error function and simple gradient descent, early stopping is equivalent to L2 regularization.What is the role of denoising autoencoders in deep learning, and how do they enhance the robustness of a model?Denoising autoencoder : Rather than adding a penalty to the loss function, an autoencoder that learns something useful by changing the reconstruction error term of the loss function. This can be done by adding some noise to the input image and making the autoencoder learn to remove it. By this means, the encoder will extract the most important features and learn a robust representation of the data.Denoising autoencoders creates a corrupted copy of the input by introducing some noise. This helps to avoid the autoencoders copying the input to the output without learning features about the data. These autoencoders take a partially corrupted input while training to recover the original undistorted input. The model learns a vector field for mapping the input data towards a lower dimensional manifold which describes the natural data to cancel out the added noise.How can Recurrent Neural Networks (RNNs) and Backpropagation through time (BPTT) be used to model temporal dependencies in sequential data such as speech, text and music?Neural networks imitate the function of the human brain in the fields of AI, machine learning, and deep learning, allowing computer programs to recognize patterns and solve common issues. RNNs are a type of neural network that can be used to model sequence data. RNNs, which are formed from feedforward networks, are similar to human brains in their behaviour. Thus, recurrent neural networks can anticipate sequential data in a way that other algorithms can’t.All of the inputs and outputs in standard neural networks are independent of one another, however in some circumstances, such as when predicting the next word of a phrase, the prior words are necessary, and so the previous words must be remembered. As a result, RNN was created, which used a Hidden Layer to overcome the problem. The most important component of RNN is the Hidden state, which remembers specific information about a sequence.RNNs have a Memory that stores all information about the calculations. It employs the same settings for each input since it produces the same outcome by performing the same task on all inputs or hidden layers.Working of RNN:-The information in recurrent neural networks cycles through a loop to the middle-hidden layer.The input layer x receives and processes the neural network’s input before passing it on to the middle layer. Multiple hidden layers can be found in the middle layer h, each with its own activation functions, weights, and biases. A recurrent neural network can be used if the various parameters of different hidden layers are not impacted by the preceding layer, i.e. There is no memory in the neural network.The different activation functions, weights, and biases will be standardized by the Recurrent Neural Network, ensuring that each hidden layer has the same characteristics. Rather than constructing numerous hidden layers, it will create only one and loop over it as many times as necessary. Backpropagation through TimeThe below figure depicts the architecture of RNN.The Backpropagation for training such networks with a slight change. BBTT doesn't independently train the network at a specific time \"t.\" Instead it trains  at a particular time \"t\" as well as all that has happened before time\"t\" like t-1, t-2, t-3. S1, S2, S3 are the hidden states at time t1, t2, t3, respectively, and Ws is the associated weight matrix x1, x2, x3are the inputs at time t1, t2, t3, respectively, and Wxis the associated weight matrix. Y1, Y2, Y3 are the outcomes at time t1, t2, t3, respectively, and Wy is the associated weight matrix. At time t0, the  input x0 is fed to the network and output y0. At time t1, we provide input x1 to the network and receive an output y1. The network uses input x and the cell state from the previous timestamp. To calculate specific Hidden state and output at each step, here is the formula:To calculate the error, we take the output and calculate its error concerning the actual result, but we have multiple outputs at each timestamp.Thus, the regular Backpropagation won't work here. Therefore, the algorithm is modified and called as Backpropagation through time. Ws, Wx, and Wy do not change across the timestamps, which means that for all inputs in a sequence, the values of these weights are the same. The Error Function Is Defined As:The total loss calculated as the sum in overall timestamps, i.e., E0+E1+E2+E3+...Explain difference between Convolutional Neural Networks (CNN’s), Recurrent Neural Networks (RNN’s) and Long ShortTerm Memory (LSTM’s)Explain the concept of greedy layer wise pre-training, and how it is used to improve the training of deep neural networks?Training deep neural networks with many layers was challenging. As the number of hidden layers is increased, the amount of error information propagated back to earlier layers is dramatically reduced. This means that weights in hidden layers close to the output layer are updated normally, whereas weights in hidden layers close to the input layer are updated minimally or not at all. Generally, this problem prevented the training of very deep neural networks and was referred to as the vanishing gradient problem.An important milestone in the resurgence of neural networking that initially allowed the development of deeper neural network models was the technique of greedy layer-wise pretraining, often simply referred to as “pretraining.”Greedy Layer-Wise Unsupervised Pretraining relies on single-layer representation learning algorithm. Each layer is pretrained using unsupervised learning, taking the output of previous layer and producing as output a new representation of the data, whose distribution is hopefully simpler. Greedy layer wise pre trainingGready: Optimize each piece of the solution independently, on piece at a time.Layer-Wise: The independent pieces are the layer of the network. Training proceeds once layer at a time, training the k-th layer while keeping the previous ones fixed.Pretraining:1st step/phase: Greedy Layer-Wise Unsupervised Pre TrainingFine-Tune all the layers together\n"
     ]
    }
   ],
   "source": [
    "docu=\"\"\n",
    "for para in document.paragraphs:\n",
    "    docu+=para.text\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of paragraph 0 is: How can Convolutional Neural Networks (CNNs) be used to extract high-level features from image and video data, and what are some of the key technical challenges involved in this process?\n",
      "\n",
      "The content of paragraph 1 is: \n",
      "\n",
      "The content of paragraph 2 is: Convolutional Neural Networks (CNNs) are a type of deep learning model that are highly effective for image and video processing. They automatically learn hierarchical representations of visual data, allowing them to extract both low-level and high-level features. \n",
      "\n",
      "The content of paragraph 3 is: MLP‟s use one perceptron for each input (e.g. pixel in an image, multiplied by 3 in RGB case). The amount of weights rapidly becomes unmanageable for large images. For a 224 x 224 pixel image with 3 colour channels there are around 150,528 weights that must be trained. As a result, difficulties arise while training and overfitting can occur. There are several techniques used to avoid overfitting in CNNs, some of which are specific to CNN architectures such as Regularization (L1/L2),  Dropout, Batch normalization, Early stopping etc. The Convolutional neural network (CNN) model for image and video processing involves the following steps.\n",
      "\n",
      "The content of paragraph 4 is: \n",
      "\n",
      "The content of paragraph 5 is: A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and is used mainly for image processing, classification, and segmentation. \n",
      "\n",
      "The content of paragraph 6 is: \n",
      "\n",
      "The content of paragraph 7 is: Input layer: The input to a cnn, is mostly an image( nxmx1-gray scale image and nxmx3- colored image).\n",
      "\n",
      "The content of paragraph 8 is: \n",
      "\n",
      "The content of paragraph 9 is: \n",
      "\n",
      "The content of paragraph 10 is: \n",
      "\n",
      "The content of paragraph 11 is: Convolution layer: it basically defines filters and we compute the convolution between the defined filters and each of the 3 images.\n",
      "\n",
      "The content of paragraph 12 is: \n",
      "\n",
      "The content of paragraph 13 is: \n",
      "\n",
      "The content of paragraph 14 is: In the same way it is applied to the remaining image i.e blue and green for RGB image (above is for red image). It is possible to apply more than one filter. More filters can be used to preserve better spatial dimensions. \n",
      "\n",
      "The content of paragraph 15 is: Convolution is used instead of considering flatten images as input to optimise the parameter and to reduce computational expense. Eg. We require 25 weights if we take a 5x5x1 image without convolution. We require 16 weights (n-f+1 x n-f+1) if we take a 5x5x1 image with a 2x2 convolution filter. Also by using convolution we can prevent overfitting of the model. It is worth having the ReLU activation function in the convolution layer which passes only positive values and negative values to zeros. \n",
      "\n",
      "The content of paragraph 16 is: \n",
      "\n",
      "The content of paragraph 17 is: Pooling layer: Pooling layer objective is to reduce the spatial dimensions of the data propagating through the network. There can be different types of pooling applied to CNN model\n",
      "\n",
      "The content of paragraph 18 is: \n",
      "\n",
      "The content of paragraph 19 is: \n",
      "\n",
      "The content of paragraph 20 is: \n",
      "\n",
      "The content of paragraph 21 is: \n",
      "\n",
      "The content of paragraph 22 is: 1. Max Pooling is the most common, for each section of the image we scan and keep the highest value. \n",
      "\n",
      "The content of paragraph 23 is: \n",
      "\n",
      "The content of paragraph 24 is: \n",
      "\n",
      "The content of paragraph 25 is: \n",
      "\n",
      "The content of paragraph 26 is: Max. Pooling provides spatial variance which enables the neural network to recognize objects in an image even if the object does not exactly resemble the original object. 2 \n",
      "\n",
      "The content of paragraph 27 is: \n",
      "\n",
      "The content of paragraph 28 is: 2. Average Pooling: Here, we take average of area we scan.\n",
      "\n",
      "The content of paragraph 29 is: \n",
      "\n",
      "The content of paragraph 30 is: \n",
      "\n",
      "The content of paragraph 31 is: \n",
      "\n",
      "The content of paragraph 32 is: Fully Connected Layer: Here, we flatten the output of the last convolutional layer and connect every node of the current layer with every other node of the next layer. This layer basically takes output of the preceding layer, whether it is a convolutional layer, ReLU or Pooling layer and outputs an n-dimensional vector, where n is the number of classes pertaining to the problem.\n",
      "\n",
      "The content of paragraph 33 is:  Fig. Fully Connected Layer\n",
      "\n",
      "The content of paragraph 34 is: \n",
      "\n",
      "The content of paragraph 35 is: \n",
      "\n",
      "The content of paragraph 36 is: Discuss the significance of using advanced activation functions in neural networks. How do alternatives like ReLU and variants address the limitations of traditional functions? \n",
      "\n",
      "The content of paragraph 37 is: \n",
      "\n",
      "The content of paragraph 38 is: Activation Functions are an extremely important feature of the Artificial Neural Network. They basically decide whether a neuron should be activated or not. It limits the output signal to a finite value.  \n",
      "\n",
      "The content of paragraph 39 is: Activation Function does the non-linear transformation to the input making it capable to learn more complex relation between input and output. It makes the network capable of learning more complex patterns. \n",
      "\n",
      "The content of paragraph 40 is: Without an activation function, the neural network is just a linear regression model as it performs only summation of product of input and weights. \n",
      "\n",
      "The content of paragraph 41 is: Activation function must be efficient and it should reduce the computation time because the neural network is sometimes trained on millions of data points.\n",
      "\n",
      "The content of paragraph 42 is: The Activation Functions can be basically divided into 3 types\n",
      "\n",
      "The content of paragraph 43 is: \n",
      "\n",
      "The content of paragraph 44 is: 1. Binary step Activation Function \n",
      "\n",
      "The content of paragraph 45 is: 2. Linear Activation Function \n",
      "\n",
      "The content of paragraph 46 is: 3. Non-linear Activation Functions\n",
      "\n",
      "The content of paragraph 47 is: \n",
      "\n",
      "The content of paragraph 48 is: ReLU (Rectified Linear Unit) activation is important in neural networks for several reasons, especially when compared to other activation functions like sigmoid, tanh, or even variants like Leaky ReLU.\n",
      "\n",
      "The content of paragraph 49 is: \n",
      "\n",
      "The content of paragraph 50 is: ReLU is widely used because it is simple, computationally efficient, helps mitigate the vanishing gradient problem, and generally leads to faster training and better performance in deep neural networks.\n",
      "\n",
      "The content of paragraph 51 is: \n",
      "\n",
      "The content of paragraph 52 is: ReLU(Rectified Linear Unit): \n",
      "\n",
      "The content of paragraph 53 is: \n",
      "\n",
      "The content of paragraph 54 is: The ReLU is the most used activation function. It is used in almost all convolutional neural networks in hidden layers only. \n",
      "\n",
      "The content of paragraph 55 is: The ReLU is half rectified(from bottom). \n",
      "\n",
      "The content of paragraph 56 is: f(z) = 0, if z < 0 \n",
      "\n",
      "The content of paragraph 57 is:          = z, otherwise \n",
      "\n",
      "The content of paragraph 58 is: R(z) = max(0,z) \n",
      "\n",
      "The content of paragraph 59 is: The range is 0 to inf. \n",
      "\n",
      "The content of paragraph 60 is: \n",
      "\n",
      "The content of paragraph 61 is: Advantages  \n",
      "\n",
      "The content of paragraph 62 is: \n",
      "\n",
      "The content of paragraph 63 is: Avoids vanishing gradient problem.  \n",
      "\n",
      "The content of paragraph 64 is: Computationally efficient—allows the network to converge very quickly\n",
      "\n",
      "The content of paragraph 65 is: Non-linear—although it looks like a linear function, ReLU has a derivative function and allows for backpropagation.\n",
      "\n",
      "The content of paragraph 66 is: \n",
      "\n",
      "The content of paragraph 67 is: Disadvantages  \n",
      "\n",
      "The content of paragraph 68 is: It can only be used with a hidden layer  \n",
      "\n",
      "The content of paragraph 69 is: Hard to train on small datasets and need much data for learning nonlinear behaviour..  \n",
      "\n",
      "The content of paragraph 70 is: The Dying ReLU problem—when inputs approach zero, or are negative, the gradient of the function becomes zero, the network cannot perform backpropagation and cannot learn.\n",
      "\n",
      "The content of paragraph 71 is: \n",
      "\n",
      "The content of paragraph 72 is: \n",
      "\n",
      "The content of paragraph 73 is: All the negative values are converted into zero, and this conversion rate is so fast that neither it can map nor fit into data properly which creates a problem.\n",
      "\n",
      "The content of paragraph 74 is: \n",
      "\n",
      "The content of paragraph 75 is: \n",
      "\n",
      "The content of paragraph 76 is: Leaky ReLU Activation Function \n",
      "\n",
      "The content of paragraph 77 is: The Leaky ReLU activation function solves the “Dying ReLU‟ problem. Leaky ReLU do not make all negative inputs to zero but to a value near to zero which solves the major issue of ReLU activation function. \n",
      "\n",
      "The content of paragraph 78 is: \n",
      "\n",
      "The content of paragraph 79 is: Advantages  \n",
      "\n",
      "The content of paragraph 80 is: Prevents dying ReLU problem—this variation of ReLU has a small positive slope in the negative area, so it does enable backpropagation, even for negative input values  Otherwise like ReLU \n",
      "\n",
      "The content of paragraph 81 is: Disadvantages  \n",
      "\n",
      "The content of paragraph 82 is: Results not consistent—leaky ReLU does not provide consistent predictions for negative input values.\n",
      "\n",
      "The content of paragraph 83 is: \n",
      "\n",
      "The content of paragraph 84 is: \n",
      "\n",
      "The content of paragraph 85 is: \n",
      "\n",
      "The content of paragraph 86 is: \n",
      "\n",
      "The content of paragraph 87 is: \n",
      "\n",
      "The content of paragraph 88 is: \n",
      "\n",
      "The content of paragraph 89 is: \n",
      "\n",
      "The content of paragraph 90 is: \n",
      "\n",
      "The content of paragraph 91 is: \n",
      "\n",
      "The content of paragraph 92 is: \n",
      "\n",
      "The content of paragraph 93 is: \n",
      "\n",
      "The content of paragraph 94 is: Explain Long Short term Memory (LSTM). What are some of the key technical challenges involved in this process?\n",
      "\n",
      "The content of paragraph 95 is: \n",
      "\n",
      "The content of paragraph 96 is: Steps Involved in LSTM Networks: \n",
      "\n",
      "The content of paragraph 97 is: \n",
      "\n",
      "The content of paragraph 98 is: Step 1: Decide how much past data it should remember \n",
      "\n",
      "The content of paragraph 99 is: The first step in the LSTM is to decide which information should be omitted from the cell in that particular time step. The sigmoid function determines this. It looks at the previous state (ht-1) along with the current input xt and computes the function.\n",
      "\n",
      "The content of paragraph 100 is: \n",
      "\n",
      "The content of paragraph 101 is: \n",
      "\n",
      "The content of paragraph 102 is: Step 2: Decide how much this unit adds to the current state\n",
      "\n",
      "The content of paragraph 103 is: \n",
      "\n",
      "The content of paragraph 104 is: In the second layer, there are two parts. One is the sigmoid function, and the other is the tanh function. In the sigmoid function, it decides which values to let through (0 or 1). tanh function gives weightage to the values which are passed, deciding their level of importance (-1 to 1). \n",
      "\n",
      "The content of paragraph 105 is: \n",
      "\n",
      "The content of paragraph 106 is: Step 3: Decide what part of the current cell state makes it to the output\n",
      "\n",
      "The content of paragraph 107 is: The third step is to decide what the output will be. First, we run a sigmoid layer, which decides what parts of the cell state make it to the output. Then, we put the cell state through tanh to push the values to be between -1 and 1 and multiply it by the output of the sigmoid gate. \n",
      "\n",
      "The content of paragraph 108 is: \n",
      "\n",
      "The content of paragraph 109 is: How does Batch Normalization work, and how can we use this technique to improve the training stability and generalization performance of Deep Learning models?\n",
      "\n",
      "The content of paragraph 110 is: \n",
      "\n",
      "The content of paragraph 111 is: Batch Normalization is a normalization technique done between the layers of a Neural Network instead of in the raw data. It is done along mini-batches instead of the full data set. \n",
      "\n",
      "The content of paragraph 112 is: It serves to speed up training and use higher learning rates, making learning easier.   Normalization formula of Batch Normalization is defined  as:\n",
      "\n",
      "The content of paragraph 113 is: \n",
      "\n",
      "The content of paragraph 114 is: \n",
      "\n",
      "The content of paragraph 115 is: Where mz is the mean of the neurons’ output and szt is the standard deviation of the neurons’ output. \n",
      "\n",
      "The content of paragraph 116 is: Thus Normalization is the process of altering the input data to have mean as zero and standard deviation value as one.\n",
      "\n",
      "The content of paragraph 117 is: \n",
      "\n",
      "The content of paragraph 118 is: Procedure to do Batch Normalization: \n",
      "\n",
      "The content of paragraph 119 is: Step 1:-\n",
      "\n",
      "The content of paragraph 120 is: Consider the batch input from layer h, for this layer we need to calculate the mean of this hidden activation. \n",
      "\n",
      "The content of paragraph 121 is: Step 2:-\n",
      "\n",
      "The content of paragraph 122 is: After calculating the mean the next step is to calculate the standard deviation of the hidden activations. \n",
      "\n",
      "The content of paragraph 123 is: Step 3:-\n",
      "\n",
      "The content of paragraph 124 is: Now we normalize the hidden activations using these Mean & Standard Deviation values. To do this, we subtract the mean from each input and divide the whole value with the sum of standard deviation and the smoothing term (ε). \n",
      "\n",
      "The content of paragraph 125 is: Step 4:-\n",
      "\n",
      "The content of paragraph 126 is: As the final stage, the re-scaling and offsetting of the input is performed. Here two components of the BN algorithm are used, γ(gamma) and β (beta). These parameters are used for re-scaling (γ) and shifting(β) the vector containing values from the previous operations. These two parameters are learnable parameters, Hence during the training of neural networks, the optimal values of γ and β are obtained and used. Hence we get the accurate normalization of each batch.\n",
      "\n",
      "The content of paragraph 127 is: \n",
      "\n",
      "The content of paragraph 128 is: Batch Normalization improve the training stability and generalization performance of Deep Learning models with the help of its several benefits, which includes\n",
      "\n",
      "The content of paragraph 129 is: It enables higher learning rates: \n",
      "\n",
      "The content of paragraph 130 is: In a non-normalized network, a large learning rate can lead to oscillations and cause the loss function increase rather than decrease. Batch Normalization helps prevents these problems by preventing small changes in the parameters from amplifying into larger and sub-optimal changes in activations and gradients. Higher learning rates in turn speed up the training process considerably. \n",
      "\n",
      "The content of paragraph 131 is: It enables better Gradient Propagation through the network, thus enabling DLNs with more hidden layers. \n",
      "\n",
      "The content of paragraph 132 is: It helps to reduce strong dependencies on the parameter initialization values. \n",
      "\n",
      "The content of paragraph 133 is: It helps to regularize the model. \n",
      "\n",
      "The content of paragraph 134 is:  Batch Normalization reduces the need to use other regularization techniques such     as Dropout.\n",
      "\n",
      "The content of paragraph 135 is: \n",
      "\n",
      "The content of paragraph 136 is: \n",
      "\n",
      "The content of paragraph 137 is: What are some of the key advantages and limitations of popular activation functions and how can we develop better activation functions that are more effective for specific applications? \n",
      "\n",
      "The content of paragraph 138 is: \n",
      "\n",
      "The content of paragraph 139 is: Neurons are responsible for making decisions. According to the activation function, the neurons determine whether to make a linear or nonlinear decision. Neurons can not learn with just a linear function attached to it. A non-linear activation function learns as per the difference w.r.t error. \n",
      "\n",
      "The content of paragraph 140 is: The main terminologies needed to understand for nonlinear functions are: \n",
      "\n",
      "The content of paragraph 141 is: Derivative or Differential: Change in y-axis w.r.t. change in x-axis.It is also known as slope. \n",
      "\n",
      "The content of paragraph 142 is: Monotonic function: A function which is either entirely non-increasing or non-decreasing.\n",
      "\n",
      "The content of paragraph 143 is: A nonlinear function which is both differential and monotonic is a better activation function and it can be  more effective for specific applications.\n",
      "\n",
      "The content of paragraph 144 is: \n",
      "\n",
      "The content of paragraph 145 is: Non-linear activation functions are crucial in neural networks because they enable the network to model complex, non-linear relationships between the input and output, allowing the network to learn more sophisticated patterns. The most commonly used non-linear activation functions are\n",
      "\n",
      "The content of paragraph 146 is: \n",
      "\n",
      "The content of paragraph 147 is: 1. Sigmoid(Logistic AF)(σ): \n",
      "\n",
      "The content of paragraph 148 is: \n",
      "\n",
      "The content of paragraph 149 is: The main reason to use the sigmoid function is it exists between 0 to 1. It is especially used for models where we have to predict the probability as output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
      "\n",
      "The content of paragraph 150 is: \n",
      "\n",
      "The content of paragraph 151 is: \n",
      "\n",
      "The content of paragraph 152 is: \n",
      "\n",
      "The content of paragraph 153 is: Advantages\n",
      "\n",
      "The content of paragraph 154 is: Easy to understand and apply \n",
      "\n",
      "The content of paragraph 155 is: Easy to train on small dataset \n",
      "\n",
      "The content of paragraph 156 is: Smooth gradient, preventing “jumps” in output values. \n",
      "\n",
      "The content of paragraph 157 is: Output values bound between 0 and 1, normalizing the output of each neuron.\n",
      "\n",
      "The content of paragraph 158 is: Disadvantages:  \n",
      "\n",
      "The content of paragraph 159 is: Vanishing gradient—for very high or very low values of X, there is almost no change to the prediction, causing a vanishing gradient problem. This can result in the network refusing to learn further, or being too slow to reach an accurate prediction.  \n",
      "\n",
      "The content of paragraph 160 is: Outputs not zero centered.  \n",
      "\n",
      "The content of paragraph 161 is: Computationally expensive \n",
      "\n",
      "The content of paragraph 162 is: \n",
      "\n",
      "The content of paragraph 163 is: 2. TanH(Hyperbolic Tangent AF)\n",
      "\n",
      "The content of paragraph 164 is: \n",
      "\n",
      "The content of paragraph 165 is: TanH is also like logistic sigmoid but in a better way. The range of the TanH function is from -1 to +1. \n",
      "\n",
      "The content of paragraph 166 is: TanH is often preferred over the sigmoid neuron because it is zero centred. The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in tanh graph.\n",
      "\n",
      "The content of paragraph 167 is: \n",
      "\n",
      "The content of paragraph 168 is: Advantages  \n",
      "\n",
      "The content of paragraph 169 is: Zero centered—making it easier to model inputs that have strongly negative, neutral, and strongly positive values. \n",
      "\n",
      "The content of paragraph 170 is: Disadvantages  \n",
      "\n",
      "The content of paragraph 171 is: Like the Sigmoid function is also suffers from vanishing gradient problem  hard to train on small datasets\n",
      "\n",
      "The content of paragraph 172 is: \n",
      "\n",
      "The content of paragraph 173 is: 3. Softmax:  \n",
      "\n",
      "The content of paragraph 174 is: Softmax is an activation function that scales numbers/logits into probabilities. The output of a Softmax is a vector with probabilities of each possible outcome. The probabilities in vector sums to one for all possible outcomes or classes. \n",
      "\n",
      "The content of paragraph 175 is: Sigmoid is able to handle more than two cases(class label).  Softmax can handle multiple cases. Softmax function squeezes the output for each class between 0 and 1 with sum of them is 1. \n",
      "\n",
      "The content of paragraph 176 is:  \n",
      "\n",
      "The content of paragraph 177 is: \n",
      "\n",
      "The content of paragraph 178 is: Advantages \n",
      "\n",
      "The content of paragraph 179 is: Able to handle multiple classes only one class in other activation functions—normalizes the outputs for each class between 0 and 1 with the sum of the probabilities being equal to 1, and divides by their sum, giving the probability of the input value being in a specific class. \n",
      "\n",
      "The content of paragraph 180 is: Useful for output neurons—typically Softmax is used only for the output layer, for neural networks that need to classify inputs into multiple categories.\n",
      "\n",
      "The content of paragraph 181 is: \n",
      "\n",
      "The content of paragraph 182 is: \n",
      "\n",
      "The content of paragraph 183 is: Discuss the concept of early stopping as a regularization technique in deep learning?\n",
      "\n",
      "The content of paragraph 184 is: \n",
      "\n",
      "The content of paragraph 185 is: Early stopping is one of the most commonly used strategies because it is very simple and quite effective. \n",
      "\n",
      "The content of paragraph 186 is: It refers to the process of stopping the training when the training error is no longer decreasing but the validation error is starting to rise.\n",
      "\n",
      "The content of paragraph 187 is: Early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. \n",
      "\n",
      "The content of paragraph 188 is: \n",
      "\n",
      "The content of paragraph 189 is: \n",
      "\n",
      "The content of paragraph 190 is: This implies the process of storing the trainable parameters periodically and tracking the validation error. After the training stops, it returns the trainable parameters to the exact point where the validation error started to rise, instead of the last ones. \n",
      "\n",
      "The content of paragraph 191 is: A different way to think of early stopping is as a very efficient hyperparameter selection algorithm, which sets the number of epochs to the absolute best. It essentially restricts the optimization procedure to a small volume of the trainable parameters space close to the initial parameters\n",
      "\n",
      "The content of paragraph 192 is: It can also be proven that in the case of a simple linear model with a quadratic error function and simple gradient descent, early stopping is equivalent to L2 regularization.\n",
      "\n",
      "The content of paragraph 193 is: \n",
      "\n",
      "The content of paragraph 194 is: \n",
      "\n",
      "The content of paragraph 195 is: What is the role of denoising autoencoders in deep learning, and how do they enhance the robustness of a model?\n",
      "\n",
      "The content of paragraph 196 is: \n",
      "\n",
      "The content of paragraph 197 is: Denoising autoencoder : Rather than adding a penalty to the loss function, an autoencoder that learns something useful by changing the reconstruction error term of the loss function. This can be done by adding some noise to the input image and making the autoencoder learn to remove it. By this means, the encoder will extract the most important features and learn a robust representation of the data.\n",
      "\n",
      "The content of paragraph 198 is: \n",
      "\n",
      "The content of paragraph 199 is: \n",
      "\n",
      "The content of paragraph 200 is: \n",
      "\n",
      "The content of paragraph 201 is: \n",
      "\n",
      "The content of paragraph 202 is: Denoising autoencoders creates a corrupted copy of the input by introducing some noise. This helps to avoid the autoencoders copying the input to the output without learning features about the data. These autoencoders take a partially corrupted input while training to recover the original undistorted input. The model learns a vector field for mapping the input data towards a lower dimensional manifold which describes the natural data to cancel out the added noise.\n",
      "\n",
      "The content of paragraph 203 is: \n",
      "\n",
      "The content of paragraph 204 is: How can Recurrent Neural Networks (RNNs) and Backpropagation through time (BPTT) be used to model temporal dependencies in sequential data such as speech, text and music?\n",
      "\n",
      "The content of paragraph 205 is: \n",
      "\n",
      "The content of paragraph 206 is: Neural networks imitate the function of the human brain in the fields of AI, machine learning, and deep learning, allowing computer programs to recognize patterns and solve common issues. RNNs are a type of neural network that can be used to model sequence data. RNNs, which are formed from feedforward networks, are similar to human brains in their behaviour. Thus, recurrent neural networks can anticipate sequential data in a way that other algorithms can’t.\n",
      "\n",
      "The content of paragraph 207 is: \n",
      "\n",
      "The content of paragraph 208 is: \n",
      "\n",
      "The content of paragraph 209 is: All of the inputs and outputs in standard neural networks are independent of one another, however in some circumstances, such as when predicting the next word of a phrase, the prior words are necessary, and so the previous words must be remembered. As a result, RNN was created, which used a Hidden Layer to overcome the problem. The most important component of RNN is the Hidden state, which remembers specific information about a sequence.\n",
      "\n",
      "The content of paragraph 210 is: RNNs have a Memory that stores all information about the calculations. It employs the same settings for each input since it produces the same outcome by performing the same task on all inputs or hidden layers.\n",
      "\n",
      "The content of paragraph 211 is: \n",
      "\n",
      "The content of paragraph 212 is: Working of RNN:-\n",
      "\n",
      "The content of paragraph 213 is: The information in recurrent neural networks cycles through a loop to the middle-hidden layer.\n",
      "\n",
      "The content of paragraph 214 is: \n",
      "\n",
      "The content of paragraph 215 is: \n",
      "\n",
      "The content of paragraph 216 is: The input layer x receives and processes the neural network’s input before passing it on to the middle layer. Multiple hidden layers can be found in the middle layer h, each with its own activation functions, weights, and biases. A recurrent neural network can be used if the various parameters of different hidden layers are not impacted by the preceding layer, i.e. There is no memory in the neural network.\n",
      "\n",
      "The content of paragraph 217 is: The different activation functions, weights, and biases will be standardized by the Recurrent Neural Network, ensuring that each hidden layer has the same characteristics. Rather than constructing numerous hidden layers, it will create only one and loop over it as many times as necessary. \n",
      "\n",
      "The content of paragraph 218 is: \n",
      "\n",
      "The content of paragraph 219 is: \n",
      "\n",
      "The content of paragraph 220 is: Backpropagation through Time\n",
      "\n",
      "The content of paragraph 221 is: \n",
      "\n",
      "The content of paragraph 222 is: The below figure depicts the architecture of RNN.\n",
      "\n",
      "The content of paragraph 223 is: \n",
      "\n",
      "The content of paragraph 224 is: \n",
      "\n",
      "The content of paragraph 225 is: The Backpropagation for training such networks with a slight change. BBTT doesn't independently train the network at a specific time \"t.\" Instead it trains  at a particular time \"t\" as well as all that has happened before time\"t\" like t-1, t-2, t-3. S1, S2, S3 are the hidden states at time t1, t2, t3, respectively, and Ws is the associated weight matrix x1, x2, x3are the inputs at time t1, t2, t3, respectively, and Wxis the associated weight matrix. Y1, Y2, Y3 are the outcomes at time t1, t2, t3, respectively, and Wy is the associated weight matrix. At time t0, the  input x0 is fed to the network and output y0. At time t1, we provide input x1 to the network and receive an output y1. The network uses input x and the cell state from the previous timestamp. To calculate specific Hidden state and output at each step, here is the formula:\n",
      "\n",
      "The content of paragraph 226 is: \n",
      "\n",
      "The content of paragraph 227 is: To calculate the error, we take the output and calculate its error concerning the actual result, but we have multiple outputs at each timestamp.Thus, the regular Backpropagation won't work here. Therefore, the algorithm is modified and called as Backpropagation through time. \n",
      "\n",
      "The content of paragraph 228 is: Ws, Wx, and Wy do not change across the timestamps, which means that for all inputs in a sequence, the values of these weights are the same. \n",
      "\n",
      "The content of paragraph 229 is: The Error Function Is Defined As:\n",
      "\n",
      "The content of paragraph 230 is: \n",
      "\n",
      "The content of paragraph 231 is: The total loss calculated as the sum in overall timestamps, i.e., E0+E1+E2+E3+...\n",
      "\n",
      "The content of paragraph 232 is: \n",
      "\n",
      "The content of paragraph 233 is: \n",
      "\n",
      "The content of paragraph 234 is: \n",
      "\n",
      "The content of paragraph 235 is: \n",
      "\n",
      "The content of paragraph 236 is: \n",
      "\n",
      "The content of paragraph 237 is: \n",
      "\n",
      "The content of paragraph 238 is: Explain difference between Convolutional Neural Networks (CNN’s), Recurrent Neural Networks (RNN’s) and Long ShortTerm Memory (LSTM’s)\n",
      "\n",
      "The content of paragraph 239 is: \n",
      "\n",
      "The content of paragraph 240 is: \n",
      "\n",
      "The content of paragraph 241 is: \n",
      "\n",
      "The content of paragraph 242 is: \n",
      "\n",
      "The content of paragraph 243 is: \n",
      "\n",
      "The content of paragraph 244 is: Explain the concept of greedy layer wise pre-training, and how it is used to improve the training of deep neural networks?\n",
      "\n",
      "The content of paragraph 245 is: \n",
      "\n",
      "The content of paragraph 246 is: Training deep neural networks with many layers was challenging. As the number of hidden layers is increased, the amount of error information propagated back to earlier layers is dramatically reduced. This means that weights in hidden layers close to the output layer are updated normally, whereas weights in hidden layers close to the input layer are updated minimally or not at all. Generally, this problem prevented the training of very deep neural networks and was referred to as the vanishing gradient problem.\n",
      "\n",
      "The content of paragraph 247 is: An important milestone in the resurgence of neural networking that initially allowed the development of deeper neural network models was the technique of greedy layer-wise pretraining, often simply referred to as “pretraining.”\n",
      "\n",
      "The content of paragraph 248 is: Greedy Layer-Wise Unsupervised Pretraining relies on single-layer representation learning algorithm. Each layer is pretrained using unsupervised learning, taking the output of previous layer and producing as output a new representation of the data, whose distribution is hopefully simpler. \n",
      "\n",
      "The content of paragraph 249 is: Greedy layer wise pre training\n",
      "\n",
      "The content of paragraph 250 is: Gready: Optimize each piece of the solution independently, on piece at a time.\n",
      "\n",
      "The content of paragraph 251 is: Layer-Wise: The independent pieces are the layer of the network. Training proceeds once layer at a time, training the k-th layer while keeping the previous ones fixed.\n",
      "\n",
      "The content of paragraph 252 is: Pretraining:\n",
      "\n",
      "The content of paragraph 253 is: 1st step/phase: Greedy Layer-Wise Unsupervised Pre Training\n",
      "\n",
      "The content of paragraph 254 is: Fine-Tune all the layers together\n",
      "\n",
      "The content of paragraph 255 is: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(document.paragraphs)):\n",
    "    print(\"The content of paragraph \"+ str(i)+\" is: \"+ document.paragraphs[i].text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Obtaining dependency information for bs4 from https://files.pythonhosted.org/packages/51/bb/bf7aab772a159614954d84aa832c129624ba6c32faa559dfb200a534e50b/bs4-0.0.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Obtaining dependency information for beautifulsoup4 from https://files.pythonhosted.org/packages/f9/49/6abb616eb3cbab6a7cca303dc02fdf3836de2e0b834bf966a7f5271a34d8/beautifulsoup4-4.13.3-py3-none-any.whl.metadata\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Obtaining dependency information for soupsieve>1.2 from https://files.pythonhosted.org/packages/d1/c2/fe97d779f3ef3b15f05c94a2f1e3d21732574ed441687474db9d342a7315/soupsieve-2.6-py3-none-any.whl.metadata\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\kalyani\\onedrive\\desktop\\nlp\\myenv\\lib\\site-packages (from beautifulsoup4->bs4) (4.12.2)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "   ---------------------------------------- 0.0/186.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/186.0 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/186.0 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 41.0/186.0 kB 495.5 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 143.4/186.0 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 186.0/186.0 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.13.3 bs4-0.0.2 soupsieve-2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
